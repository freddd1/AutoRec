{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# AutoRec cs3639 Recommendation Systems course IDC"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### here will be general explanations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch\n",
    "from torch import nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this project, we will use 2 datasets:\n",
    "* **movielens**, which can be downloaded using `utils.datasets_download.py` or straight from [here](http://files.grouplens.org/datasets/movielens/).\n",
    "* **netflixprize**, which can be downloaded from this [semi-parsed version from kaggle](https://www.kaggle.com/netflix-inc/netflix-prize-data) or from this [raw version](https://archive.org/download/nf_prize_dataset.tar)\n",
    "\n",
    "**NOTE**: for the notebook to run properly, you should save you dataset under `data` folder and `movielens` folder for the movielens dataset and `netflix` folder for the netflixprize dataset.\n",
    "i.e `data/movielens` folder and `data/netflix` folder respectively."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from src.mf.model import MatrixFactorization\n",
    "from src.mf.training import MFTrainer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": "       user_id  item_id  rating  timestamp\n0            1        1       5  874965758\n1            1        2       3  876893171\n2            1        3       4  878542960\n3            1        4       3  876893119\n4            1        5       3  889751712\n...        ...      ...     ...        ...\n79995      943     1067       2  875501756\n79996      943     1074       4  888640250\n79997      943     1188       3  888640250\n79998      943     1228       3  888640275\n79999      943     1330       3  888692465\n\n[80000 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>item_id</th>\n      <th>rating</th>\n      <th>timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n      <td>874965758</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>876893171</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>3</td>\n      <td>4</td>\n      <td>878542960</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>4</td>\n      <td>3</td>\n      <td>876893119</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>5</td>\n      <td>3</td>\n      <td>889751712</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>79995</th>\n      <td>943</td>\n      <td>1067</td>\n      <td>2</td>\n      <td>875501756</td>\n    </tr>\n    <tr>\n      <th>79996</th>\n      <td>943</td>\n      <td>1074</td>\n      <td>4</td>\n      <td>888640250</td>\n    </tr>\n    <tr>\n      <th>79997</th>\n      <td>943</td>\n      <td>1188</td>\n      <td>3</td>\n      <td>888640250</td>\n    </tr>\n    <tr>\n      <th>79998</th>\n      <td>943</td>\n      <td>1228</td>\n      <td>3</td>\n      <td>888640275</td>\n    </tr>\n    <tr>\n      <th>79999</th>\n      <td>943</td>\n      <td>1330</td>\n      <td>3</td>\n      <td>888692465</td>\n    </tr>\n  </tbody>\n</table>\n<p>80000 rows Ã— 4 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data_prep import movielens_load\n",
    "train, test = movielens_load(1)\n",
    "print(train.shape)\n",
    "train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1: Avg losses: train: 6.981, val: 5.174\n",
      "EPOCH 2: Avg losses: train: 4.361, val: 3.969\n",
      "EPOCH 3: Avg losses: train: 3.678, val: 3.689\n",
      "EPOCH 4: Avg losses: train: 3.476, val: 3.502\n",
      "EPOCH 5: Avg losses: train: 3.174, val: 3.088\n",
      "EPOCH 6: Avg losses: train: 2.604, val: 2.525\n",
      "EPOCH 7: Avg losses: train: 2.208, val: 2.311\n",
      "EPOCH 8: Avg losses: train: 2.106, val: 2.267\n",
      "EPOCH 9: Avg losses: train: 2.092, val: 2.268\n",
      "EPOCH 10: Avg losses: train: 2.092, val: 2.262\n",
      "EPOCH 11: Avg losses: train: 2.089, val: 2.265\n",
      "EPOCH 12: Avg losses: train: 2.094, val: 2.261\n",
      "EPOCH 13: Avg losses: train: 2.088, val: 2.274\n",
      "EPOCH 14: Avg losses: train: 2.093, val: 2.270\n",
      "EPOCH 15: Avg losses: train: 2.094, val: 2.266\n",
      "EPOCH 16: Avg losses: train: 2.095, val: 2.264\n",
      "EPOCH 17: Avg losses: train: 2.092, val: 2.264\n",
      "EPOCH 18: Avg losses: train: 2.092, val: 2.270\n",
      "EPOCH 19: Avg losses: train: 2.093, val: 2.263\n",
      "EPOCH 20: Avg losses: train: 2.094, val: 2.253\n",
      "EPOCH 21: Avg losses: train: 2.088, val: 2.266\n",
      "EPOCH 22: Avg losses: train: 2.092, val: 2.266\n",
      "EPOCH 23: Avg losses: train: 2.091, val: 2.268\n",
      "EPOCH 24: Avg losses: train: 2.091, val: 2.264\n",
      "EPOCH 25: Avg losses: train: 2.087, val: 2.265\n",
      "EPOCH 26: Avg losses: train: 2.092, val: 2.265\n",
      "EPOCH 27: Avg losses: train: 2.090, val: 2.263\n",
      "EPOCH 28: Avg losses: train: 2.089, val: 2.264\n",
      "EPOCH 29: Avg losses: train: 2.088, val: 2.260\n",
      "EPOCH 30: Avg losses: train: 2.090, val: 2.267\n",
      "EPOCH 31: Avg losses: train: 2.089, val: 2.261\n",
      "EPOCH 32: Avg losses: train: 2.089, val: 2.260\n",
      "EPOCH 33: Avg losses: train: 2.088, val: 2.261\n",
      "EPOCH 34: Avg losses: train: 2.087, val: 2.261\n",
      "EPOCH 35: Avg losses: train: 2.089, val: 2.256\n",
      "EPOCH 36: Avg losses: train: 2.088, val: 2.268\n",
      "EPOCH 37: Avg losses: train: 2.087, val: 2.260\n",
      "EPOCH 38: Avg losses: train: 2.087, val: 2.262\n",
      "EPOCH 39: Avg losses: train: 2.090, val: 2.264\n",
      "EPOCH 40: Avg losses: train: 2.085, val: 2.264\n",
      "EPOCH 41: Avg losses: train: 2.089, val: 2.265\n",
      "EPOCH 42: Avg losses: train: 2.088, val: 2.255\n",
      "EPOCH 43: Avg losses: train: 2.085, val: 2.258\n",
      "EPOCH 44: Avg losses: train: 2.090, val: 2.253\n",
      "EPOCH 45: Avg losses: train: 2.084, val: 2.258\n",
      "EPOCH 46: Avg losses: train: 2.088, val: 2.259\n",
      "EPOCH 47: Avg losses: train: 2.085, val: 2.266\n",
      "EPOCH 48: Avg losses: train: 2.090, val: 2.257\n",
      "EPOCH 49: Avg losses: train: 2.086, val: 2.259\n",
      "EPOCH 50: Avg losses: train: 2.083, val: 2.268\n",
      "EPOCH 51: Avg losses: train: 2.088, val: 2.260\n",
      "EPOCH 52: Avg losses: train: 2.086, val: 2.258\n",
      "EPOCH 53: Avg losses: train: 2.083, val: 2.261\n",
      "EPOCH 54: Avg losses: train: 2.085, val: 2.269\n",
      "EPOCH 55: Avg losses: train: 2.087, val: 2.257\n",
      "EPOCH 56: Avg losses: train: 2.086, val: 2.254\n",
      "EPOCH 57: Avg losses: train: 2.084, val: 2.255\n",
      "EPOCH 58: Avg losses: train: 2.083, val: 2.259\n",
      "EPOCH 59: Avg losses: train: 2.084, val: 2.265\n",
      "EPOCH 60: Avg losses: train: 2.085, val: 2.260\n",
      "EPOCH 61: Avg losses: train: 2.083, val: 2.262\n",
      "EPOCH 62: Avg losses: train: 2.084, val: 2.259\n",
      "EPOCH 63: Avg losses: train: 2.085, val: 2.257\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [5]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m model \u001B[38;5;241m=\u001B[39m MatrixFactorization(num_users, num_items, k\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m80\u001B[39m)\n\u001B[0;32m      4\u001B[0m mf_trainer \u001B[38;5;241m=\u001B[39m MFTrainer(train, test, model, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m150\u001B[39m, lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.002\u001B[39m, reg\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m128\u001B[39m)\n\u001B[1;32m----> 5\u001B[0m \u001B[43mmf_trainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\DataspellProjects\\AutoRec\\src\\mf\\training.py:65\u001B[0m, in \u001B[0;36mMFTrainer.train_model\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_epoch()\n\u001B[0;32m     64\u001B[0m \u001B[38;5;66;03m# Evaluate\u001B[39;00m\n\u001B[1;32m---> 65\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meval_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     67\u001B[0m \u001B[38;5;66;03m# Print epoch statistics\u001B[39;00m\n\u001B[0;32m     68\u001B[0m train_avg \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mnanmean(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_losses[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n",
      "File \u001B[1;32m~\\DataspellProjects\\AutoRec\\src\\mf\\training.py:116\u001B[0m, in \u001B[0;36mMFTrainer.eval_epoch\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m batch_id, (users_ids, items_ids, ratings) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mval_loader):\n\u001B[0;32m    115\u001B[0m         \u001B[38;5;66;03m# Set data\u001B[39;00m\n\u001B[1;32m--> 116\u001B[0m         users_ids \u001B[38;5;241m=\u001B[39m \u001B[43musers_ids\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    117\u001B[0m         items_ids \u001B[38;5;241m=\u001B[39m items_ids\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m    118\u001B[0m         ratings \u001B[38;5;241m=\u001B[39m ratings\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)  \u001B[38;5;66;03m# rating is our label\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "num_users = train.user_id.max()\n",
    "num_items = train.item_id.max()\n",
    "model = MatrixFactorization(num_users, num_items, k=80)\n",
    "mf_trainer = MFTrainer(train, test, model, epochs=150, lr=0.002, reg=0.001, batch_size=128)\n",
    "mf_trainer.train_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from src.data_prep import movielens_create_ratings\n",
    "train, test = movielens_create_ratings(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from src.autorec.model import AutoRec\n",
    "from src.autorec.training import AutoRecTrainer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "num_users, num_items = train.shape\n",
    "model = AutoRec(num_hidden=512, num_features=num_users)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1: Avg losses: train: 1.631, val: 1.096\n",
      "EPOCH 2: Avg losses: train: 1.018, val: 0.982\n",
      "EPOCH 3: Avg losses: train: 0.951, val: 0.971\n",
      "EPOCH 4: Avg losses: train: 0.918, val: 0.926\n",
      "EPOCH 5: Avg losses: train: 0.894, val: 0.919\n",
      "EPOCH 6: Avg losses: train: 0.871, val: 0.899\n",
      "EPOCH 7: Avg losses: train: 0.858, val: 0.888\n",
      "EPOCH 8: Avg losses: train: 0.842, val: 0.887\n",
      "EPOCH 9: Avg losses: train: 0.830, val: 0.867\n",
      "EPOCH 10: Avg losses: train: 0.816, val: 0.860\n",
      "EPOCH 11: Avg losses: train: 0.806, val: 0.861\n",
      "EPOCH 12: Avg losses: train: 0.795, val: 0.850\n",
      "EPOCH 13: Avg losses: train: 0.785, val: 0.850\n",
      "EPOCH 14: Avg losses: train: 0.783, val: 0.848\n",
      "EPOCH 15: Avg losses: train: 0.775, val: 0.857\n",
      "EPOCH 16: Avg losses: train: 0.765, val: 0.834\n",
      "EPOCH 17: Avg losses: train: 0.755, val: 0.833\n",
      "EPOCH 18: Avg losses: train: 0.749, val: 0.812\n",
      "EPOCH 19: Avg losses: train: 0.741, val: 0.818\n",
      "EPOCH 20: Avg losses: train: 0.733, val: 0.826\n",
      "EPOCH 21: Avg losses: train: 0.725, val: 0.824\n",
      "EPOCH 22: Avg losses: train: 0.723, val: 0.813\n",
      "EPOCH 23: Avg losses: train: 0.714, val: 0.820\n",
      "EPOCH 24: Avg losses: train: 0.709, val: 0.827\n",
      "EPOCH 25: Avg losses: train: 0.708, val: 0.813\n",
      "EPOCH 26: Avg losses: train: 0.703, val: 0.804\n",
      "EPOCH 27: Avg losses: train: 0.696, val: 0.804\n",
      "EPOCH 28: Avg losses: train: 0.688, val: 0.811\n",
      "EPOCH 29: Avg losses: train: 0.683, val: 0.794\n",
      "EPOCH 30: Avg losses: train: 0.675, val: 0.806\n",
      "EPOCH 31: Avg losses: train: 0.669, val: 0.799\n",
      "EPOCH 32: Avg losses: train: 0.664, val: 0.807\n",
      "EPOCH 33: Avg losses: train: 0.661, val: 0.782\n",
      "EPOCH 34: Avg losses: train: 0.658, val: 0.797\n",
      "EPOCH 35: Avg losses: train: 0.650, val: 0.794\n",
      "EPOCH 36: Avg losses: train: 0.648, val: 0.790\n",
      "EPOCH 37: Avg losses: train: 0.646, val: 0.791\n",
      "EPOCH 38: Avg losses: train: 0.640, val: 0.776\n",
      "EPOCH 39: Avg losses: train: 0.635, val: 0.775\n",
      "EPOCH 40: Avg losses: train: 0.630, val: 0.787\n",
      "EPOCH 41: Avg losses: train: 0.630, val: 0.788\n",
      "EPOCH 42: Avg losses: train: 0.626, val: 0.784\n",
      "EPOCH 43: Avg losses: train: 0.622, val: 0.769\n",
      "EPOCH 44: Avg losses: train: 0.619, val: 0.780\n",
      "EPOCH 45: Avg losses: train: 0.614, val: 0.773\n",
      "EPOCH 46: Avg losses: train: 0.610, val: 0.779\n",
      "EPOCH 47: Avg losses: train: 0.611, val: 0.778\n",
      "EPOCH 48: Avg losses: train: 0.609, val: 0.766\n",
      "EPOCH 49: Avg losses: train: 0.604, val: 0.771\n",
      "EPOCH 50: Avg losses: train: 0.600, val: 0.773\n",
      "EPOCH 51: Avg losses: train: 0.598, val: 0.767\n",
      "EPOCH 52: Avg losses: train: 0.596, val: 0.765\n",
      "EPOCH 53: Avg losses: train: 0.596, val: 0.767\n",
      "EPOCH 54: Avg losses: train: 0.593, val: 0.768\n",
      "EPOCH 55: Avg losses: train: 0.590, val: 0.762\n",
      "EPOCH 56: Avg losses: train: 0.589, val: 0.770\n",
      "EPOCH 57: Avg losses: train: 0.588, val: 0.768\n",
      "EPOCH 58: Avg losses: train: 0.583, val: 0.761\n",
      "EPOCH 59: Avg losses: train: 0.578, val: 0.776\n",
      "EPOCH 60: Avg losses: train: 0.574, val: 0.773\n",
      "EPOCH 61: Avg losses: train: 0.579, val: 0.768\n",
      "EPOCH 62: Avg losses: train: 0.573, val: 0.774\n",
      "EPOCH 63: Avg losses: train: 0.572, val: 0.760\n",
      "EPOCH 64: Avg losses: train: 0.570, val: 0.747\n",
      "EPOCH 65: Avg losses: train: 0.567, val: 0.751\n",
      "EPOCH 66: Avg losses: train: 0.568, val: 0.764\n",
      "EPOCH 67: Avg losses: train: 0.577, val: 0.758\n",
      "EPOCH 68: Avg losses: train: 0.566, val: 0.768\n",
      "EPOCH 69: Avg losses: train: 0.565, val: 0.764\n",
      "EPOCH 70: Avg losses: train: 0.565, val: 0.760\n",
      "EPOCH 71: Avg losses: train: 0.562, val: 0.762\n",
      "EPOCH 72: Avg losses: train: 0.560, val: 0.757\n",
      "EPOCH 73: Avg losses: train: 0.556, val: 0.756\n",
      "EPOCH 74: Avg losses: train: 0.558, val: 0.759\n",
      "EPOCH 75: Avg losses: train: 0.556, val: 0.745\n",
      "EPOCH 76: Avg losses: train: 0.551, val: 0.749\n",
      "EPOCH 77: Avg losses: train: 0.555, val: 0.763\n",
      "EPOCH 78: Avg losses: train: 0.554, val: 0.768\n",
      "EPOCH 79: Avg losses: train: 0.551, val: 0.752\n",
      "EPOCH 80: Avg losses: train: 0.546, val: 0.755\n",
      "EPOCH 81: Avg losses: train: 0.549, val: 0.754\n",
      "EPOCH 82: Avg losses: train: 0.553, val: 0.749\n",
      "EPOCH 83: Avg losses: train: 0.549, val: 0.761\n",
      "EPOCH 84: Avg losses: train: 0.548, val: 0.762\n",
      "EPOCH 85: Avg losses: train: 0.544, val: 0.759\n",
      "EPOCH 86: Avg losses: train: 0.546, val: 0.753\n",
      "EPOCH 87: Avg losses: train: 0.542, val: 0.737\n",
      "EPOCH 88: Avg losses: train: 0.540, val: 0.744\n",
      "EPOCH 89: Avg losses: train: 0.542, val: 0.750\n",
      "EPOCH 90: Avg losses: train: 0.539, val: 0.750\n",
      "EPOCH 91: Avg losses: train: 0.537, val: 0.759\n",
      "EPOCH 92: Avg losses: train: 0.536, val: 0.751\n",
      "EPOCH 93: Avg losses: train: 0.537, val: 0.744\n",
      "EPOCH 94: Avg losses: train: 0.535, val: 0.747\n",
      "EPOCH 95: Avg losses: train: 0.534, val: 0.763\n",
      "EPOCH 96: Avg losses: train: 0.530, val: 0.756\n",
      "EPOCH 97: Avg losses: train: 0.531, val: 0.744\n",
      "EPOCH 98: Avg losses: train: 0.528, val: 0.746\n",
      "EPOCH 99: Avg losses: train: 0.530, val: 0.746\n",
      "EPOCH 100: Avg losses: train: 0.535, val: 0.752\n"
     ]
    }
   ],
   "source": [
    "autorec_trainer = AutoRecTrainer(train, test, model, epochs=100, batch_size=64, lr=0.001, reg=0.001)\n",
    "autorec_trainer.train_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}